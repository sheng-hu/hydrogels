# Data-Driven De Novo Design of Super-Adhesive Hydrogels
### Descriptions
- This is the code for reproducing the submission "Data-Driven De Novo Design of Super-Adhesive Hydrogels."
### Datasets
- The datasets are all in Excel forms with a .xlsx extension in this directory.
- For example:
```
Original Data_ML_20220829.xlsx
```

### Requirements and Tutorials
The codes are tested using Python 3.8.3, on Ubuntu 18.04.
You can use the codes below to install all requirements and run our codes.
1. Install Miniconda:
   Please download the installer of Miniconda and install it with the instructions from the following URLs.
   - Windows
     https://docs.conda.io/projects/conda/en/stable/user-guide/install/windows.html
   - MacOS
     https://docs.conda.io/projects/conda/en/stable/user-guide/install/macos.html
   - Linux
     https://docs.conda.io/projects/conda/en/stable/user-guide/install/linux.html
2. Entering your terminal:
   Windows/MacOS/Linux have their own terminal applications; you can choose your favorite one. (Windows users need to use WSL.)
3. Download this repository and enter the directory:
   Use the Download button on this page to download the repository and rename it as `hydrogels`, then use `cd hydrogels/` command to enter this directory.
4. Installing dependencies:
   Run the following commands:
   ```
   pip install -r requirement.txt
   ```
5. Finished:
   Now, you are ready to run our codes to reproduce.

(Typical install time: <5 min)

### Usage
Use the commands below for extrapolation for F_a (Adhesive Strength) optimization.
- GP_CLmax: adopting y_max (i.e., maximum F_a in the training set) as a hypothetical value when deciding the next data points of EI maximums
```
python CLmax_gp.py
```
- GP_CLmin: adopting y_min (i.e., minimum F_a in the training set) as a hypothetical value when deciding the next data points of EI maximums
```
python CLmin_gp.py
```
- GP_LP: adopting a local penalized term in EI calculation
```
python LP_gp.py
```
- GP_KB: adopting GP predictions as hypothetical values when deciding the next data points of EI maximums
```
python gp_gp.py
```
- GP-RFR: adopting trained GP as a hypothetical value provider and RFR as an EI maximizer when deciding the next data points of EI maximums
```
python gp_rfr.py
```
- RFR-RFR: adopting trained RFR as a hypothetical value provider and RFR as an EI maximizer when deciding the next data points of EI maximum
```
python rfr_rfr.py
```
- RFR-GP: adopting trained RFR as a hypothetical value provider and GP as an EI maximizer when deciding the next data points of EI maximums
```
python rfr_gp.py
```
- RFR-GP*: a variant of RFR-GP with a warm-start, for which 10 data points randomly generated by trained RFR are added into real data for GP regression
```
python rfr_gp_star.py
```
- Expected Results:
The output should be an id list followed by a list of formula logs.
```
[ 41 106 197 183 195   3 196 182 187 185  40 212 213 207 211 210 153 194
 209 190 192 163 199 206 219 193 189 151 188 200 201 202 214   4 198 152
 215 180 107  39 217 204  12  73  11  80 191 140 203 218 186 166 103  34
 184  58 139 181 216 111 114 208  78 205 162  59 141  50  75  62  95  72
 119  60  97 104 148 130  96  83  33   7 108 136   9 110  74  71 131 109
  25  52 129  17  28  47  26  14  24  84  37 102 169  53 126 116 173  23
  21 120  15  44  42  54  81  20  51  18  69 100 174  67  29  19  10  36
 122  76 137  43  61 115   6  32  30  66 155  46  49 145 176  45 101 149
 132  38 128  79  91  90   0 121 117  31  13  70 170 165 127  27  55 158
 125 146 161  35 134 154 150 118 144 147  48 168 160 175 124   1 159  92
  87 123 143 142 113   8   2  65  16  82  94  57 167  63 177  64  86 164
 112 171   5  56  22 178 156  93  89 157 138 133  88  99  68 172 105 135
  77  98 179  85]
41 [0.14, 0.59, 0.08, 0.1, 0.1, 0.0] ML predicted: 146.63835810332625
106 [0.13, 0.59, 0.09, 0.09, 0.1, 0.0] ML predicted: 137.0134465675867
197 [0.0, 0.67, 0.0, 0.1, 0.16, 0.07] ML predicted: 127.8218163623182
183 [0.0, 0.67, 0.0, 0.1, 0.17, 0.06] ML predicted: 127.21261010273318
195 [0.0, 0.68, 0.0, 0.1, 0.17, 0.05] ML predicted: 126.99168848581459
3 [0.22, 0.2, 0.09, 0.17, 0.32, 0.0] ML predicted: 126.82236376503893
196 [0.0, 0.67, 0.0, 0.1, 0.23, 0.0] ML predicted: 125.4575461640517
182 [0.0, 0.68, 0.0, 0.1, 0.21, 0.0] ML predicted: 125.32589026318524
```
Expected running time: <10 min

### Further evaluations for round2&3
```
python rfr_gp_round2.py
python rfr_gp_round3.py
python gp_gp_round2.py
python gp_gp_round3.py
```

### Reproduction instructions
Due to the numeric randomness from the SMBO implementation of scikit-optimize, we rerun the scripts for 1000 times to collect the most frequent formulas and use them for wet experiment validations in our paper.

To generate the same formulas used in our paper, we provide the log files that repeated for 1000 times.
The log files and scripts can be found in `reproduce` directory.

- Example
```
# In round 1 SMBO process, show only 50 results sorting by EI with formula frequency > cutoff
python count_logs_by_EI.py --round_num=1 --cutoff=50 --show_num=50 rfr_gp_saved_models.log
```
```
# In round 1 SMBO process, show only 50 results sorting by PRED with formula frequency > cutoff
python count_logs_by_PRED.py --round_num=1 --cutoff=50 --show_num=50 rfr_gp_saved_models.log
```
```
# In round 2 SMBO process, show only 50 results sorting by EI with formula frequency > cutoff
python count_logs_by_EI.py --round_num=2 --cutoff=50 --show_num=50 rfr_gp_round2_saved_models.log
```
```
# In round 2 SMBO process, show only 50 results sorting by PRED with formula frequency > cutoff
python count_logs_by_PRED.py --round_num=2 --cutoff=50 --show_num=50 rfr_gp_round2_saved_models.log
```
```
# In round 3 SMBO process, show only 50 results sorting by EI with formula frequency > cutoff
python count_logs_by_EI.py --round_num=3 --cutoff=50 --show_num=50 rfr_gp_round3_saved_models.log
```
```
# In round 3 SMBO process, show only 50 results sorting by PRED with formula frequency > cutoff
python count_logs_by_PRED.py --round_num=3 --cutoff=50 --show_num=50 rfr_gp_round3_saved_models.log
```

We show the results for benchmarking ML models in jupyter notebooks:

The correlation analysis is also in `round1_180.ipynb`.

```
round1_180.ipynb
round2_289.ipynb
round3_316.ipynb
round4_341.ipynb
```
  








